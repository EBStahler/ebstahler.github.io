<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="VAEs for Instrument Audio Generation">
  <link rel="shortcut icon" href="images/icon.ico">
  <title>Reconstructing Resonance: VAEs for Instrument Audio Generation</title>
  <!-- Link to the external CSS file -->
  <link rel="stylesheet" href="main.css">
</head>
<body>
  <header>
    <h1>Reconstructing Resonance: VAEs for Instrument Audio Generation</h1>
    <p>Final Project for 6.7960, MIT - By Ellery Stahler & Evan Bell</p>
  </header>

  <div class="content-container">
    <!-- Left Sidebar -->
    <div class="left-sidebar">
      <h2>Outline</h2>
      <ul>
        <li><a href="#intro">Background and Movtivation</a></li>
        <li><a href="#data">Data and preprocessing</a></li>
        <li><a href="#future">Future Works</a></li>
        <li><a href="#Appendix">Appendix</a></li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <!-- Main Content -->
    <div class="main-content">

      <section id="intro">
        <h1>Background and Movtivation</h1>
        <p>
          Why Should We Care About VAEs in Music?
          Highlight the rise of machine learning in creative domains.
          Discuss how VAEs are transforming generative tasks, particularly in music, where rhythm, harmony, and timbre intertwine.
          Pose a question: How can we leverage VAEs for not only creating music but also transferring musical features across styles and domains?
        
        <h3>What are VAEs?</h3>
        <p>
          Variational Autoencoders (VAEs) are a powerful class of generative models that learn a probabilistic mapping from data to a lower-dimensional latent space. They consist of two components: an encoder and a decoder. The encoder transforms the input data (such as an image, audio sample, or text) into a distribution over the latent space, typically modeled as a multivariate Gaussian with mean and variance parameters. The decoder then samples from this distribution and reconstructs the input data from the latent code. This setup allows the model to generate new data by sampling from the latent space, rather than directly reconstructing it from a fixed encoding.
        </p>
        <p>
          The key motivation behind VAEs lies in their ability to impose a structure on the latent space. In traditional autoencoders, the latent representation is learned in an unsupervised manner, but it lacks any explicit probabilistic structure, which makes it difficult to sample meaningful new data from the latent space. VAEs overcome this by introducing a regularization term (the Kullback-Leibler (KL) divergence) in the loss function, which forces the learned latent distribution to approximate a prior, typically a standard normal distribution. This regularization encourages smoothness and continuity in the latent space, ensuring that similar points in the latent space correspond to similar data points. As a result, VAEs make it easy to sample from the latent space and generate novel data points that resemble the training data, a crucial feature for generative tasks like image synthesis, music generation, and other forms of data creation.
        </p>

        <h3>Related Works</h3>
        <p>
          Variational Autoencoders (VAEs) have been widely studied in the image domain, where tasks like image generation and transfer learning have demonstrated the impact of hyperparameter choices, such as latent space dimension and encoder depth. For example, research on <strong>beta-VAEs</strong> has shown that tuning the latent space size can significantly affect disentanglement, enabling greater control over generated outputs. Additionally, studies have highlighted the benefits of partially retraining model components, like the decoder, to improve performance during domain transfer.
        </p>
        <p>
          In the audio domain, VAEs have been applied to tasks like <strong>speech synthesis</strong>, <strong>music generation</strong>, and <strong>timbre transfer</strong>. Projects such as Google Brain’s <em>NSynth</em> demonstrated how VAEs can generate new musical tones by blending characteristics of different instruments. However, the focus has primarily been on generation rather than transfer learning. Manipulating latent spaces for attributes like rhythm or timbre has shown promise, but how these features transfer across domains—such as from one genre to another—remains underexplored.
        </p>
        <p>
          Audio data poses unique challenges compared to images due to its <strong>temporal structure</strong> and <strong>spectral complexity</strong>. Unlike images, which are spatially static, audio evolves over time and spans multiple frequencies simultaneously, requiring models to capture both temporal and spectral features effectively. Existing work has not systematically examined how VAEs perform transfer learning across audio distributions, such as between instruments or genres.
        </p>
        <p>
          This project aims to bridge this gap by investigating how hyperparameters, particularly <strong>latent dimension size</strong> and <strong>encoder/decoder depth</strong>, influence transfer learning performance in VAEs for audio. It also seeks to compare these findings with results in the image domain, shedding light on the unique challenges posed by audio data. By addressing these questions, this work contributes to understanding the potential of VAEs for transfer learning in audio and highlights key differences between audio and image domains.
        </p>

      <h3>Introduction</h3>
        <p>
          The intersection of machine learning and music has seen significant advancements in recent years, with Variational Autoencoders (VAEs) emerging as powerful tools for generating music.
          VAEs, a class of generative models, learn compact latent representations of complex data, enabling the generation of high-quality, diverse music.
          By training on a dataset of musical compositions, VAEs can capture underlying patterns in rhythm, harmony, and melody, allowing for the creation of new compositions that mirror the structure of the training data.
        </p>
        <p>
          In this project, we explore the application of VAEs to the task of music generation.
          The goal is to leverage the flexibility and expressiveness of VAEs to generate novel pieces of music, focusing on generating musical structures such as melodies, harmonies, and rhythms.
          By using VAEs, we aim to not only produce music that is coherent and musically engaging but also to explore the latent space of music, uncovering new combinations and patterns that may not be immediately apparent from the training data.
        </p>
        <p>
          The project builds on previous work in audio synthesis and music generation using VAEs, such as their application in generating speech and musical sequences.
          By utilizing VAEs in the context of music, we can push the boundaries of generative music models, offering a deeper understanding of how machine learning can be used to create art in the form of music.
        </p>
        <p>
          This work also investigates how the structure of the VAE's latent space can be manipulated to produce specific musical styles or variations, providing musicians and composers with new creative tools for exploring the possibilities of automated music creation.
        </p>
      </section>

      

      <section id="Data">
        <h1>Data and preprocessing</h1>
        <p>
          For this project, we used audio data from the Medley-solos-DB, a database containing 21,572 mono audio clips sampled at 44.1 kHz with a 32-bit depth. Each audio clip has a fixed duration of 2972 milliseconds, or 65536 discrete-time samples, providing a uniform and manageable input size for our model. These clips are split by instrument, allowing us to isolate individual instruments for training and generation tasks.
        </p>
        <p>
          To prepare the data for training, we performed several preprocessing steps. The audio data was first converted into spectrograms, which provide a time-frequency representation of the audio. This transformation is crucial for working with audio in machine learning, as raw waveform data can be difficult for models to process effectively.
        </p>
        <p>
          A spectrogram is essentially a visual representation of the frequencies that are present in an audio signal over time. It is computed by applying a Fourier transform to the audio signal, breaking it down into its frequency components across small windows of time. The Fourier transform itself is a mathematical operation that converts a time-domain signal (which shows how the signal varies with time) into a frequency-domain signal (which shows how much of each frequency is present at any given time).
        </p>
        <p>
          To create the spectrograms, we used the following parameters:
          <ul>
            <li>n_mels = 512: This sets the number of Mel-frequency bins, which are a logarithmic scale that mimics the human ear's sensitivity to sound. The Mel scale is designed to capture the perceptually significant frequencies, and 512 bins offer a good balance between frequency resolution and computational efficiency.</li>
            <li>n_fft = 4096: This is the size of the FFT window. A larger window size results in higher frequency resolution, which is useful for capturing fine details in the frequency domain.</li>
            <li>hop_len = 1024: The hop length is the number of samples between successive frames. A smaller hop length allows for finer temporal resolution, which is important for capturing the dynamics of music.</li>
            <li>sr = 22050: The sample rate of the spectrogram was set to 22,050 Hz, which is commonly used in music-related tasks for balancing time and frequency resolution.</li>
          </ul>
        </p>
        <p>
          Using these settings, we generated Mel spectrograms with the dimensions (512, 64), where 512 corresponds to the number of frequency bins (on the Mel scale) and 64 represents the number of time frames, effectively reducing the dimensionality of the raw audio while retaining crucial frequency and temporal information. These spectrograms were then used as input data for training our Variational Autoencoder (VAE) model.
          The goal of this preprocessing was to create spectrograms that are small in dimension but informative enough for robust reconstruction during the VAE training process. This was important for ensuring that the latent space of the VAE could capture the musical features needed to generate high-quality audio outputs.
        </p>

        <figure class="iframe-container">
          <iframe
          blog_template\
            src="audio_single_img.html?img_filename=media/spect_example/example_spec.png&audio_filename=media/spect_example/example_audio.wav&width=400&height=200"
            style="border: none; width: 400px; height: 200px;"></iframe>
          </iframe>
          <hr>
          <figcaption>An example Mel spectorgram sample from the latent space <br>(NOTE: Click image to play audio)</figcaption>
        </figure>

      </section>

      <section>
        <h3>Model Architechtures and Loss Variations</h3>

        <p>
          We explore four main kinds of encoder/decoder architectures. In particular, we consider a fully linear model, which
          flattens the input image, then has a series of linear layers, each decreasing in width, until it reaches the desired latent dimension.
          We then consider a CNN-based encoder and decoder which contains a series of convolutional layers, downsizing the image while also creating
          convolutional embeddings. The result is then flattened and linear layers are applied in order to reach the desired latent dimension.
          The next architecture that we tested is a more domain-aware CNN approach. In particular, 
          
          <br>
          <br>
          WRITE MORE ABOUT HOW WE EDITED THE MODELS

          <br>

          <figure style="text-align: center; padding: 16px; background-color: #f9f9f9; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); width: 80%; margin: 20px auto;">
            <img 
              src="media/loss_betas.png" 
              alt="Description of the image" 
              style="max-width: 100%; height: auto; border-radius: 8px;">
            <figcaption style="margin-top: 12px; font-size: 16px; font-weight: bold; color: #555;">
              Training of Violin Conv Model with beta variances, losses plotted left, beta on right 
            </figcaption>
          </figure>
        </p>

    </section>

    
    <section id="experimental-results">
      <h1>Experimental Results</h1>
      <p>

        To investigate the latent dimensions, we took our best models, from ARCHITECTURE WITH a latent space of 8.
        We generated latent vectors with varied values for each pair of latent dimension indicies. 
      </p>
      <p>
        Below is an interactive plot of 
        these samples (both image and audio) for each pair of latent dimensions:
      </p>

        <figure class="iframe-container">
          <iframe
            src="folder_selector.html?rows=5&cols=5&width=500&height=500&off_w=75&off_h=62"
            style="border: none; width: 500px; height: 563px;"></iframe>
          </iframe>
          <hr>
          <figcaption>Latent Dimension Analysis for Piano samples, using MODEL ARCHITECTURE. <br>(NOTE: Click image to play audio)</figcaption>
        </figure>
  
        <figure class="iframe-container">
          <iframe
            src="compare_model.html?width=700&height=200&folder=media/model_comparisons/reconstructed_audio/&colNames=true,linear_model,resid_model,conv_model,audio_conv_model"
            style="border: none; width: 700px; height: 200px;"></iframe>
          </iframe>
        </figure>
  
        <figure class="iframe-container">
          <iframe
            src="compare_model.html?width=500&height=400&folder=media/model_comparisons/samples_from_latent/&colNames=linear_model,resid_model,conv_model,audio_conv_model"
            style="border: none; width: 500px; height: 400px;"></iframe>
          </iframe>
        </figure>
  
        <figure class="iframe-container">
          <iframe
            src="compare_model.html?width=500&height=800&rows=10&folder=media/mult_instruments_model_results/&colNames=flute,piano,violin"
            style="border: none; width: 500px; height: 800px;"></iframe>
          </iframe>
        </figure>

      </p>
    </section>



    <section id="Transfer Learning in Audio">
      <h1>Transfer Learning in Audio</h1>
      <p>
        We also briefly explored the task of transfer learning on instrument audio generation, namely through passing piano audio through the 
        flute encoder, training a linear neural network to interpolate between the produced latent space and the piano's latent space, then 
        decoding the new latent vector using the piano's decoder. This was an attempt to find out whether it can be faster to use the encoder
        of another domain (flute) to train a small latent-to-latent network (flute to piano) rather than training an entire network on piano alone.
        The base assumption is that different domains of audio share similar qualities which may be embedded in the latent space (in the domain of 
        instrument audio this is rythm, pitch, etc), so all that is needed to transfer between domains is translation between latent spaces.

        DESCRIBE MODELS HERE
      

        <figure class="iframe-container">
          <iframe
            src="compare_model.html?width=500&height=400&folder=media/transfer_learning/&colNames=true,piano_decoded,transfer_decoded"
            style="border: none; width: 500px; height: 400px;"></iframe>
            <hr>
          <div>Comparison of two transfer learning decodings, next to the original piano input</div>
        </figure>
      </p>
    </section>

      <section id="future">
        <h1>Future Work</h1>
        <p>
        
        </p>
      </section>

      <section id="implications_and_limitations">
        <h1>Implications and Limitations</h1>
        <p>
          Discuss the implications and limitations of your project.
        </p>
      </section>


      <section id="Appendix">
        <h1>Appendix</h1>

        <b>A1</b>
        <table id = "Appendix-Table-1">
          <caption>Linear Model Architecture for Encoder and Decoder</caption>
          <thead>
            <tr>
              <th>Layer Type</th>
              <th>Layer Name</th>
              <th>Input Dimension</th>
              <th>Output Dimension</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Encoder</td>
              <td>Flatten</td>
              <td>(512x64)</td>
              <td>32768</td>
              <td>Flatten spectrogram</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear1</td>
              <td>32768</td>
              <td>4096</td>
              <td></td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear2</td>
              <td>4096</td>
              <td>1024</td>
              <td></td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear3</td>
              <td>1024</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_mu</td>
              <td>256</td>
              <td>8</td>
              <td>Encode mu</td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_logvar</td>
              <td>256</td>
              <td>8</td>
              <td>Encode logvar</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear1</td>
              <td>8</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear2</td>
              <td>256</td>
              <td>1024</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear3</td>
              <td>1024</td>
              <td>4096</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear1</td>
              <td>4096</td>
              <td>32768</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Unflatten</td>
              <td>32768</td>
              <td>(512x64)</td>
              <td>Final reconstructed output</td>
            </tr>
          </tbody>
        </table>
        <br>
        <hr>

        
        <table id = "Appendix-Table-2">
          <b>A2</b>
          <caption>Convolutional Model Architecture for Encoder and Decoder</caption>
          <thead>
            <tr>
              <th>Layer Type</th>
              <th>Layer Name</th>
              <th>Input Dimension</th>
              <th>Output Dimension</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Encoder</td>
              <td>Conv1</td>
              <td>(1x512x64)</td>
              <td>(16x512x64)</td>
              <td>kernel_size=(3,3), stride=(1,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv2</td>
              <td>(16x512x64)</td>
              <td>(32x128x32)</td>
              <td>kernel_size=(4,4), stride=(4,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv3</td>
              <td>(32x128x32)</td>
              <td>(64x32x16)</td>
              <td>kernel_size=(4,4), stride=(4,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv4</td>
              <td>(64x32x16)</td>
              <td>(128x16x8)</td>
              <td>kernel_size=(3,3), stride=(2,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv5</td>
              <td>(128x16x8)</td>
              <td>(128x8x8)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Flatten</td>
              <td>(128x8x8)</td>
              <td>8196</td>
              <td>Flatten</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear1</td>
              <td>8196</td>
              <td>1024</td>
              <td></td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear2</td>
              <td>1024</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_mu</td>
              <td>256</td>
              <td>8</td>
              <td>Encode mu</td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_logvar</td>
              <td>256</td>
              <td>8</td>
              <td>Encode logvar</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear1</td>
              <td>8</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear2</td>
              <td>256</td>
              <td>1024</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear3</td>
              <td>1024</td>
              <td>8196</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Unflatten</td>
              <td>8196</td>
              <td>(128x8x8)</td>
              <td>Unflatten</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv1</td>
              <td>(128x8x8)</td>
              <td>(128x16x8)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv2</td>
              <td>(128x16x8)</td>
              <td>(64x32x16)</td>
              <td>kernel_size=(3,3), stride=(2,2)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv3</td>
              <td>(64x32x16)</td>
              <td>(32x128x32)</td>
              <td>kernel_size=(5,5), stride=(4,2)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv4</td>
              <td>(32x128x32)</td>
              <td>(16x512x64)</td>
              <td>kernel_size=(5,5), stride=(4,2)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv5</td>
              <td>(16x512x64)</td>
              <td>(1x512x64)</td>
              <td>kernel_size=(1,1), stride=(1,1)</td>
            </tr>
          </tbody>
        </table>
        <br>
        <hr>

        <b>A3</b>
        <table id = "Appendix-Table-3">
          <caption>Audio Domain Aware Convolutional Model Architecture for Encoder and Decoder</caption>
          <thead>
            <tr>
              <th>Layer Type</th>
              <th>Layer Name</th>
              <th>Input Dimension</th>
              <th>Output Dimension</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Encoder</td>
              <td>Conv1</td>
              <td>(1x512x64)</td>
              <td>(16x512x64)</td>
              <td>kernel_size=(3,3), stride=(1,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv2</td>
              <td>(16x512x64)</td>
              <td>(32x256x64)</td>
              <td>kernel_size=(5,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv3</td>
              <td>(32x256x64)</td>
              <td>(64x256x32)</td>
              <td>kernel_size=(1,5), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv4</td>
              <td>(64x256x32)</td>
              <td>(128x128x32)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv5</td>
              <td>(128x128x32)</td>
              <td>(128x64x32)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv6</td>
              <td>(128x64x32)</td>
              <td>(128x64x16)</td>
              <td>kernel_size=(1,3), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv7</td>
              <td>(128x64x16)</td>
              <td>(128x32x16)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv8</td>
              <td>(128x32x16)</td>
              <td>(128x16x16)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Flatten</td>
              <td>(128x16x16)</td>
              <td>32768</td>
              <td>Flatten</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear1</td>
              <td>32768</td>
              <td>2048</td>
              <td></td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear2</td>
              <td>2048</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_mu</td>
              <td>256</td>
              <td>8</td>
              <td>Encode mu</td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_logvar</td>
              <td>256</td>
              <td>8</td>
              <td>Encode logvar</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear1</td>
              <td>8</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear2</td>
              <td>256</td>
              <td>2048</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear3</td>
              <td>2048</td>
              <td>32176</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Unflatten</td>
              <td>32176</td>
              <td>(128x16x16)</td>
              <td>Unflatten</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv1</td>
              <td>(128x16x16)</td>
              <td>(128x32x16)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv2</td>
              <td>(128x32x16)</td>
              <td>(128x64x16)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv3</td>
              <td>(128x64x16)</td>
              <td>(128x64x32)</td>
              <td>kernel_size=(1,3), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv4</td>
              <td>(128x64x32)</td>
              <td>(128x128x32)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv5</td>
              <td>(128x128x32)</td>
              <td>(64x256x32)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv6</td>
              <td>(64x256x32)</td>
              <td>(32x256x64)</td>
              <td>kernel_size=(1,5), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv7</td>
              <td>(32x256x64)</td>
              <td>(16x512x64)</td>
              <td>kernel_size=(5,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv8</td>
              <td>(16x512x64)</td>
              <td>(1x512x64)</td>
              <td>kernel_size=(3,3), stride=(1,1)</td>
            </tr>
          </tbody>
        </table>
        <br>
        <hr>

        <b>A4</b>
        <table id = "Appendix-Table-4">
          <caption>Audio Domain Aware Residual Model Architecture for Encoder and Decoder</caption>
          <thead>
            <tr>
              <th>Layer Type</th>
              <th>Layer Name</th>
              <th>Input Dimension</th>
              <th>Output Dimension</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Encoder</td>
              <td>Resid1</td>
              <td>(1x512x64)</td>
              <td>(16x256x32)</td>
              <td>kernel_size=(3,3), stride=(2,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Resid2</td>
              <td>(16x256x32)</td>
              <td>(32x128x32)</td>
              <td>kernel_size=(5,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Resid3</td>
              <td>(32x128x32)</td>
              <td>(64x128x16)</td>
              <td>kernel_size=(1,5), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Resid4</td>
              <td>(64x128x16)</td>
              <td>(128x64x16)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv1</td>
              <td>(128x64x16)</td>
              <td>(128x64x8)</td>
              <td>kernel_size=(1,3), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv2</td>
              <td>(128x64x8)</td>
              <td>(128x32x8)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Conv3</td>
              <td>(128x32x8)</td>
              <td>(128x16x8)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Flatten</td>
              <td>(128x16x8)</td>
              <td>16384</td>
              <td>Flatten</td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear1</td>
              <td>16384</td>
              <td>2048</td>
              <td></td>
            </tr>
            <tr>
              <td>Encoder</td>
              <td>Linear2</td>
              <td>2048</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_mu</td>
              <td>256</td>
              <td>8</td>
              <td>Encode mu</td>
            </tr>
            <tr>
              <td>Bottleneck</td>
              <td>fc_logvar</td>
              <td>256</td>
              <td>8</td>
              <td>Encode logvar</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear1</td>
              <td>8</td>
              <td>256</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear2</td>
              <td>256</td>
              <td>2048</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Linear3</td>
              <td>2048</td>
              <td>16384</td>
              <td></td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Unflatten</td>
              <td>16384</td>
              <td>(128x16x8)</td>
              <td>Unflatten</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Resid1</td>
              <td>(128x16x8)</td>
              <td>(128x16x8)</td>
              <td>kernel_size=(3,3), stride=(1,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv1</td>
              <td>(128x16x8)</td>
              <td>(128x32x8)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv2</td>
              <td>(128x32x8)</td>
              <td>(128x64x8)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv3</td>
              <td>(128x64x8)</td>
              <td>(128x64x16)</td>
              <td>kernel_size=(1,3), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Resid2</td>
              <td>(128x64x16)</td>
              <td>(128x64x16)</td>
              <td>kernel_size=(3,3), stride=(1,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv4</td>
              <td>(128x64x16)</td>
              <td>(64x128x16)</td>
              <td>kernel_size=(3,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv5</td>
              <td>(64x128x16)</td>
              <td>(32x128x32)</td>
              <td>kernel_size=(1,5), stride=(1,2)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Resid3</td>
              <td>(32x128x32)</td>
              <td>(32x128x32)</td>
              <td>kernel_size=(3,3), stride=(1,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv6</td>
              <td>(32x128x32)</td>
              <td>(16x256x32)</td>
              <td>kernel_size=(5,1), stride=(2,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Resid4</td>
              <td>(16x256x32)</td>
              <td>(16x256x32)</td>
              <td>kernel_size=(3,3), stride=(1,1)</td>
            </tr>
            <tr>
              <td>Decoder</td>
              <td>Deconv7</td>
              <td>(16x256x32)</td>
              <td>(1x512x64)</td>
              <td>kernel_size=(3,3), stride=(2,2)</td>
            </tr>
          </tbody>
        </table>

      </section>

      <br>
      <section id="references" class="references">
        <h1>References</h1>
        <!-- <a id = "reference-1" href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">[1] Allegory of the Cave, Plato, c. 375 BC</a>
        <br>
        <a href="#">[2] A Human-Level AGI, OpenAI, 2025</a> -->
      </section>
    </div>

    <!-- Right Sidebar -->
    <!-- <div class="right-sidebar">
      <h2>Note</h2>
      <p>Add supplementary notes or additional content here.</p>
    </div> -->
  </div>

  <footer>
    &copy; 2024, Ellery Stahler and Evan Bell
  </footer>
  <!-- Link to external JavaScript file -->
  <script src="script.js"></script>
</body>
</html>
